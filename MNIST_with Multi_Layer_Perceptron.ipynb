{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MULTI-LAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-39a41629ec58>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ahmed/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ahmed/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ahmed/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ahmed/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ahmed/anaconda3/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"tmp/data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n there is 55000 images and each image is from 784 pixels\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape\n",
    "'''\n",
    " there is 55000 images and each image is from 784 pixels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist.train.images[2].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7ef9cf8810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imshow allow us to show image from matrix column \n",
    "plt.imshow(mnist.train.images[80].reshape(28,28),cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# we are going to try to make a deep learning or multi layer precepteress network to be able to just take in the pixel information here and correctly classify it as to whatever number it represents  and correctly classify it as to whatever number it represents \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some para\n",
    "#(the tree basic parametres that we are going to use later)\n",
    "    # the learning rate \n",
    "    # the number of training \n",
    "    # the patch batch size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he going to be how wuickly we adjust our cost function \n",
    "learning_rate = 0.001\n",
    "#how many training we going through \n",
    "training_epochs = 15\n",
    "# the size of the batches of training data \n",
    "batch_size =   100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have some network parameters that directly define \n",
    "# what our neural network will look like \n",
    "n_classes = 10 #(the number of output we must have 1,2,3...10)\n",
    "n_sample = mnist.train.num_examples # number of observation or input \n",
    "n_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784 # the nb pixel  of obervation we want only images of 784 pix \n",
    "# cause we have 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how  many neurons we are goiing to use \n",
    "# we use 256 because computer store image information like this \n",
    "n_hideden_l1 = 256\n",
    "n_hideden_l2 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n now we have our param next we are going to  receive our input data \\narray and send it to the first hidden layer of 256 neurons  then the data beging \\nhaving weight attached to it between the layers thats oass along  between the layers \\nalso we add a bais along then continue to the next hid layer \\ntill the end outputlayer \\nonce that transformed data that'e been multiplied by these waits had reashed the output layer \\nwe evaluate how far off  from the desires results  we are  we are goiing to use a cost or loss function\\nand check how many classes are correct and minimize this cost function by adjusting the \\nweight values.\\n\\nin our case we will use a very commun optimizer Atom optimizer \\nwe are going to see how quickly to apply this optimization by changing the learning rate \\nthat say how quickly do you want apply that optimization function\\nand the lower the rate the higher the accurate with lot of time \\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " now we have our param next we are going to  receive our input data \n",
    "array and send it to the first hidden layer of 256 neurons  then the data beging \n",
    "having weight attached to it between the layers thats oass along  between the layers \n",
    "also we add a bais along then continue to the next hid layer \n",
    "till the end outputlayer \n",
    "once that transformed data that'e been multiplied by these waits had reashed the output layer \n",
    "we evaluate how far off  from the desires results  we are  we are goiing to use a cost or loss function\n",
    "and check how many classes are correct and minimize this cost function by adjusting the \n",
    "weight values.\n",
    "\n",
    "in our case we will use a very commun optimizer Atom optimizer \n",
    "we are going to see how quickly to apply this optimization by changing the learning rate \n",
    "that say how quickly do you want apply that optimization function\n",
    "and the lower the rate the higher the accurate with lot of time \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create our function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create a model we ll stat with two hitting layers \n",
    "# and going to use the RELU activation function which is the very simple\n",
    "# rectifier function which return x or 0\n",
    "# for the final output we will unse linear activation matrix multiplication \n",
    "\n",
    "\n",
    "def multiplayer_preceptron(x,weiths,baises):\n",
    "    '''\n",
    "     x : placeholder for data input \n",
    "     weights : dic of weights\n",
    "     biases  : dic of bias values \n",
    "    '''\n",
    "    \n",
    "    #first Hiden Layer with RELU  ACtivation \n",
    "    # X * W + B \n",
    "    layer_1 = tf.add(tf.matmul(x,weights['h1']),baises['b1'])\n",
    "    # func (x*w+b) = RELU -> f(x)= max(0,x)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    #Second hidden layer \n",
    "    #first Hiden Layer with RELU  ACtivation \n",
    "    # X * W + B \n",
    "    layer_2 = tf.add(tf.matmul(layer_1,weights['h2']),baises['b2'])\n",
    "    # func (x*w+b) = RELU -> f(x)= max(0,x)\n",
    "    layer_1 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    #last output layer \n",
    "    out_layer = tf.matmul(layer_2,weights[\"out\"])+ baises[\"out\"]\n",
    "    \n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h1': <tf.Variable 'Variable_9:0' shape=(784, 256) dtype=float32_ref>,\n",
       " 'h2': <tf.Variable 'Variable_10:0' shape=(256, 256) dtype=float32_ref>,\n",
       " 'out': <tf.Variable 'Variable_11:0' shape=(256, 10) dtype=float32_ref>}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dic of weights and Bias \n",
    "# we goona introduce a new object type from tf called variable \n",
    "# and this is diferent from constant because tf has what's know as \n",
    "#graph object  that can become aware of the states of all variables \n",
    "# and variable is editable in tf \n",
    "\n",
    "weights = {\n",
    "    # cause we want wights to be random \n",
    "    \"h1\":tf.Variable(tf.random_normal([n_input,n_hideden_l1])),\n",
    "    \"h2\":tf.Variable(tf.random_normal([n_hideden_l1,n_hideden_l2])),\n",
    "    \"out\": tf.Variable(tf.random_normal([n_hideden_l2,n_classes]))\n",
    "}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "    # cause we want biases  to be random \n",
    "    # we want to be an item of a list \n",
    "    \"b1\":tf.Variable(tf.random_normal([n_hideden_l1])),\n",
    "    \"b2\":tf.Variable(tf.random_normal([n_hideden_l2])),\n",
    "    \"out\": tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last thing to do is set to placeholder for x and y \n",
    "x = tf.placeholder(\"float\",[None,n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we construct the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = multiplayer_preceptron(x,weights,biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define out cost optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next thing we will do is just train our model \n",
    "# and train our model for 15 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7ee9b69550>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAN/klEQVR4nO3df6hc9ZnH8c+T2JiQVkw2Y7gxsrdbAyqiaRlCsFIVtWhA8wOVBIlWElJEoULxB1klEYwEd7thwaVwuwnJrl1rpQ254GW32VAMBS0ZNdGodHU1msRr7mSDeqtgTfLsH/dYbuKd79zMOTNnrs/7BcPMnGfOnCeTfHJmznfOfM3dBeDrb1LZDQDoDMIOBEHYgSAIOxAEYQeCOKuTG5s1a5b39vZ2cpNAKAcOHNDRo0dtrFqusJvZDZL+WdJkSf/q7htTj+/t7VWtVsuzSQAJ1Wq1Ya3lt/FmNlnSv0i6UdIlklaY2SWtPh+A9srzmX2BpLfd/R13/4ukX0laXExbAIqWJ+znSzo46v6hbNkpzGyNmdXMrFav13NsDkAebT8a7+597l5192qlUmn35gA0kCfshyVdMOr+3GwZgC6UJ+x7JM0zs2+b2RRJyyX1F9MWgKK1PPTm7sfN7F5J/6WRobct7v56YZ0BKFSucXZ3H5A0UFAvANqIr8sCQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EERHp2xG561evTpZ37x5c7L++OOPJ+t33313sn7uuecm6+gc9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7BPA559/nqz39vY2rA0NDSXXnTQp/f/9ww8/nKy/+OKLyfqOHTuSdXROrrCb2QFJw5JOSDru7tUimgJQvCL27Ne4+9ECngdAG/GZHQgib9hd0u/M7CUzWzPWA8xsjZnVzKxWr9dzbg5Aq/KG/Up3/56kGyXdY2Y/OP0B7t7n7lV3r1YqlZybA9CqXGF398PZ9ZCk7ZIWFNEUgOK1HHYzm25m3/rytqQfStpfVGMAipXnaPxsSdvN7Mvn+Q93/89CusIpBgcHk/VmY+nt9NFHH5W2bZyZlsPu7u9IurzAXgC0EUNvQBCEHQiCsANBEHYgCMIOBMEprl3g+PHjyfqGDRs61MmZ27dvX7K+c+fOhrXrr7++6HaQwJ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0L9PX1Jetbtmxp27bPO++8ZL3Z6bPDw8PJ+rJlyxrW+vv7k+suXLgwWZ82bVqyjlOxZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhn74BmY9VPPvlk27Z9yy23JOubNm1K1p966qlk/ZFHHknWP/vss4a16667LrnuVVddlawPDAwk61OnTk3Wo2HPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM5egHq9nqxfccUVyfq7775bZDunWLt2bbI+Z86cZP2BBx5I1ufOnZusr1y5MllPef7555P1RYsWJevPPfdcw1rEc+Gb7tnNbIuZDZnZ/lHLZprZTjN7K7ue0d42AeQ1nrfxWyXdcNqyhyTtcvd5knZl9wF0saZhd/fdko6dtnixpG3Z7W2SlhTcF4CCtXqAbra7D2a3P5Q0u9EDzWyNmdXMrNbssy2A9sl9NN7dXZIn6n3uXnX3aqVSybs5AC1qNexHzKxHkrLr9GldAErXatj7Jd2Z3b5T0o5i2gHQLk3H2c3saUlXS5plZockrZO0UdKvzWyVpPck3dbOJrvdoUOHkvW84+hnn312sv7CCy80rF122WW5tt3MihUrkvXUOeW33nprrm03G4e///77G9ba+RsC3app2N290d/mtQX3AqCN+LosEARhB4Ig7EAQhB0IgrADQXCKawH27NnT1udvdjrm5Zdf3tbtp5hZsr5kSePTJrZv355c94knnkjWU0OOkrR169aW+pKa/8z1RMSeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJy9AH19fWW30LUmTWq8P7n55puT6157bfrEyp6enmT9008/bVhrNpV1s+9OzJs3L1nvRuzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnRtaZPn56sX3jhhcn6vn37GtaGh4eT66bG6Ccq9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7JiwHn300WS92W/DpzT7Tfv58+e3/NxlabpnN7MtZjZkZvtHLVtvZofNbG92WdTeNgHkNZ638Vsl3TDG8k3uPj+7DBTbFoCiNQ27u++WdKwDvQBoozwH6O41s1ezt/kzGj3IzNaYWc3MavV6PcfmAOTRath/Luk7kuZLGpT0s0YPdPc+d6+6e7VSqbS4OQB5tRR2dz/i7ifc/aSkX0haUGxbAIrWUtjNbPRv+C6VtL/RYwF0h6bj7Gb2tKSrJc0ys0OS1km62szmS3JJByT9uI09AmP64osv2vbcS5cubdtzl6Vp2N19xRiLN7ehFwBtxNdlgSAIOxAEYQeCIOxAEIQdCIJTXAvQbOrhV155pUOdxLJq1aqyW5hQ2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMsxfgpptuStab/eQx0Ans2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZJ4BPPvkkWV+3bl3D2vr165PrmlkrLRXi5MmTyXqz3oeHhwvs5uuPPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4ewEuvvjiZL3Z75tv3pyeFLfZePRjjz3WsDZt2rTkusuWLUvWe3t7k/UpU6Yk60NDQw1rzf7cGzZsSNbzWL16dbJ+0UUXtW3bZWm6ZzezC8zs92b2hpm9bmY/yZbPNLOdZvZWdj2j/e0CaNV43sYfl/RTd79E0kJJ95jZJZIekrTL3edJ2pXdB9Clmobd3Qfd/eXs9rCkNyWdL2mxpG3Zw7ZJWtKuJgHkd0YH6MysV9J3Jf1R0mx3H8xKH0qa3WCdNWZWM7NavV7P0SqAPMYddjP7pqTfSLrP3U85M8PdXZKPtZ6797l71d2rlUolV7MAWjeusJvZNzQS9F+6+2+zxUfMrCer90hqfNgVQOlsZKeceMDIOZDbJB1z9/tGLf8HSf/n7hvN7CFJM939gdRzVatVr9VqBbQ9sXzwwQfJ+qWXXpqsf/zxx0W2c0aaTUd9zjnnJOu7d+9uWHv//fdb6mm8Fi1a1LD27LPPJtedOnVq0e10RLVaVa1WG/O85fGMs39f0kpJr5nZ3mzZWkkbJf3azFZJek/SbUU0C6A9mobd3f8gqdEvHFxbbDsA2oWvywJBEHYgCMIOBEHYgSAIOxAEp7h2wJw5c5L1gYGBZH3p0qXJeuo00rz6+/vb9tx53XHHHcn6gw8+2LA2UcfR82DPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM7eBRYuXJisHzx4MFl/5plnGtbuuuuu5LonTpxI1tupWW/XXHNNsr58+fJkffLkyWfc09cZe3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9gngrLPSf0233357SzXEwp4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JoGnYzu8DMfm9mb5jZ62b2k2z5ejM7bGZ7s0vjybABlG48X6o5Lumn7v6ymX1L0ktmtjOrbXL3f2xfewCKMp752QclDWa3h83sTUnnt7sxAMU6o8/sZtYr6buS/pgtutfMXjWzLWY2o8E6a8ysZma1er2eq1kArRt32M3sm5J+I+k+d/9E0s8lfUfSfI3s+X821nru3ufuVXevViqVAloG0Ipxhd3MvqGRoP/S3X8rSe5+xN1PuPtJSb+QtKB9bQLIazxH403SZklvuvs/jVreM+phSyXtL749AEUZz9H470taKek1M9ubLVsraYWZzZfkkg5I+nFbOgRQiPEcjf+DJBujlJ5UHEBX4Rt0QBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIMzdO7cxs7qk90YtmiXpaMcaODPd2lu39iXRW6uK7O1v3X3M33/raNi/snGzmrtXS2sgoVt769a+JHprVad64208EARhB4IoO+x9JW8/pVt769a+JHprVUd6K/UzO4DOKXvPDqBDCDsQRClhN7MbzOxPZva2mT1URg+NmNkBM3stm4a6VnIvW8xsyMz2j1o208x2mtlb2fWYc+yV1FtXTOOdmGa81Neu7OnPO/6Z3cwmS/ofSddLOiRpj6QV7v5GRxtpwMwOSKq6e+lfwDCzH0j6s6R/c/dLs2VPSDrm7huz/yhnuPuDXdLbekl/Lnsa72y2op7R04xLWiLpRyrxtUv0dZs68LqVsWdfIOltd3/H3f8i6VeSFpfQR9dz992Sjp22eLGkbdntbRr5x9JxDXrrCu4+6O4vZ7eHJX05zXipr12ir44oI+znSzo46v4hddd87y7pd2b2kpmtKbuZMcx298Hs9oeSZpfZzBiaTuPdSadNM941r10r05/nxQG6r7rS3b8n6UZJ92RvV7uSj3wG66ax03FN490pY0wz/ldlvnatTn+eVxlhPyzpglH352bLuoK7H86uhyRtV/dNRX3kyxl0s+uhkvv5q26axnusacbVBa9dmdOflxH2PZLmmdm3zWyKpOWS+kvo4yvMbHp24ERmNl3SD9V9U1H3S7ozu32npB0l9nKKbpnGu9E04yr5tSt9+nN37/hF0iKNHJH/X0l/X0YPDfr6O0n7ssvrZfcm6WmNvK37QiPHNlZJ+htJuyS9Jem/Jc3sot7+XdJrkl7VSLB6SurtSo28RX9V0t7ssqjs1y7RV0deN74uCwTBATogCMIOBEHYgSAIOxAEYQeCIOxAEIQdCOL/AVdGM9oBLYY7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# there is one more convenience function to explain  that's \n",
    "# available in our mnist data object next_batch what doeas \n",
    "#you can pass in a number and will return ad tuple of batch of 10 sample \n",
    "\n",
    "t = mnist.train.next_batch(1)\n",
    "# what means it just a 10 sample of the actual training data \n",
    "Xsamp,Ysamp = t\n",
    "plt.imshow(Xsamp.reshape(28,28),cmap=\"Greys\")\n",
    "\n",
    "# to conclude next batch return the input and their label as here \n",
    "# as here the degit in the image and the label that correspond to it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ysamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## RUN THE SESSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ahmed/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 cost 219.7692\n",
      "Epoch : 2 cost 55.2873\n",
      "Epoch : 3 cost 35.2287\n",
      "Epoch : 4 cost 25.1905\n",
      "Epoch : 5 cost 18.5790\n",
      "Epoch : 6 cost 14.5779\n",
      "Epoch : 7 cost 11.4998\n",
      "Epoch : 8 cost 8.9865\n",
      "Epoch : 9 cost 7.3399\n",
      "Epoch : 10 cost 5.9399\n",
      "Epoch : 11 cost 4.9876\n",
      "Epoch : 12 cost 4.0577\n",
      "Epoch : 13 cost 3.5024\n",
      "Epoch : 14 cost 3.1145\n",
      "Epoch : 15 cost 2.4469\n",
      "Model has completed 15 epochs of training \n"
     ]
    }
   ],
   "source": [
    "# 15 loops \n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    # cost \n",
    "    avg_cost = 0.0\n",
    "    total_batch = int (n_sample/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "        # now we will u se the feed dictionary for the optimization\n",
    "        # and loss value \n",
    "        _,c = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "        # the reason we put anderscore because we will never goin to use \n",
    "        # this variable so sess run return the loss value \n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"Epoch : {} cost {:.4f}\".format(epoch+1,avg_cost))\n",
    "print(\"Model has completed {} epochs of training \".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME TO EVALUATE OUR MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.equal is a way of checking how many predication are equal to the test \n",
    "correct_prod= tf.equal(tf.argmax(pred,1),tf.arg_max(y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(correct_prod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prod = tf.cast(correct_prod,\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_prod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce mean in order to grab the mean of elements across the tensor\n",
    "accuracy = tf.reduce_mean(correct_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnits.test.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9506"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval({x:mnist.test.images,y:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
